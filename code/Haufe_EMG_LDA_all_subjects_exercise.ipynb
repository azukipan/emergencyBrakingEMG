{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code produces an EMG dataset according to the methodology described by Haufe et al. in \"EEG potentials predict upcoming emergency brakings during simulated driving.\" dx.doi.org/10.1088/1741-2560/8/5/056001. Instead of the original modeling approach, we train and test our own model here for all test subjects from the Haufe et al. study. \n",
    "\n",
    "The original EMG data are part of a dataset from Haufe et al. called \"Emergency braking during simulated driving.\" The dataset is available at http://bnci-horizon-2020.eu/database/data-sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll iterate through all the test subject data files to create our train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "from statistics import mean\n",
    "\n",
    "#Create training data from braking event EMG via these steps:\n",
    "#Get segments of braking event EMG.\n",
    "#Covert to PSD.\n",
    "#Store PSD components of each segment in variable for training.\n",
    "def createDatasetFromEMGEvents(timestamps, data, samplingRate, numberOfPSDComponents = 4):\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "    \n",
    "    dt1_index = 0\n",
    "    dt2_index = int(100/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint at 100 ms.\n",
    "    baselineCorrection_emg = mean(data[dt1_index:dt2_index+1])\n",
    "    \n",
    "    #Define variables to split data in first 1/2 for training and second 1/2 for validation\n",
    "    brakingEvent_emg_PSD_train = []\n",
    "    brakingEvent_emg_PSD_val = []\n",
    "\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "\n",
    "    for time in timestamps: #Iterate through event timestamps in milliseconds\n",
    "        #index = int(time/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint\n",
    "        dt1_index = int((time-300)/1000/dt) #Index of datapoint 300 ms before event datapoint.\n",
    "        dt2_index = int((time+1200)/1000/dt) #Index of datapoint 1200 ms after event datapoint.\n",
    "\n",
    "        brakingEvent_emg = data[dt1_index:dt2_index+1]-baselineCorrection_emg\n",
    "        #Normalize signal data WRT to max and find generate power spectral density \n",
    "        freq_data, time_data, pwr_spectral_density_data = spectrogram(\n",
    "                                                            np.array([brakingEvent_emg]),\n",
    "                                                            samplingRate\n",
    "                                                            )\n",
    "        if time < len(data)*1000*dt/2:\n",
    "            brakingEvent_emg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "            continue\n",
    "        brakingEvent_emg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "    return brakingEvent_emg_PSD_train, brakingEvent_emg_PSD_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create baseline training EMG data containing no braking event EMG via these steps:\n",
    "#Get 100 ms EMG segment at beginning of data to use for baseline correction.\n",
    "#Get segments of EMG without braking events and subract 100 ms EMG segment.\n",
    "#Covert to PSD.\n",
    "#Store PSD components of each segment in variable for training.\n",
    "\n",
    "def createDatasetFromEMGWithoutEvents(timestamps, data, samplingRate, numberOfPSDComponents=4):\n",
    "    dt = 1/samplingRate #Time increment in seconds\n",
    "    \n",
    "    dt1_index = 0\n",
    "    dt2_index = int(100/1000/dt) #Covert timestamps to seconds and divde by time increment to get index of datapoint at 100 ms.\n",
    "    baselineCorrection_emg = mean(data[dt1_index:dt2_index+1])\n",
    "\n",
    "    noEvent_emg_PSD_train = []\n",
    "    noEvent_emg_PSD_val = []\n",
    "\n",
    "    for i in range(0, len(timestamps)): #Iterate through all event timestamps in milliseconds\n",
    "        if timestamps[i][0] < 4500: #Skip iteration if there is not enough time to get an emg segment between time of first datapoint and time of first event. \n",
    "            continue\n",
    "\n",
    "        if i > 0:\n",
    "            if timestamps[i][0]-timestamps[i-1][0] < 7500: #Skip iteration if there is not enough time to get emg segment between current and previous timestamps.\n",
    "                continue\n",
    "\n",
    "        numberOfSegments = int((timestamps[i][0]-timestamps[i-1][0]-6000)/2000) #Calculate how many user-specified EMG segments can fit between two events.\n",
    "        \n",
    "        for segmentNum in range(0, numberOfSegments):\n",
    "            #Add 500 ms between each EMG segment, except for segment closest in time to event\n",
    "            dt1_index = int((timestamps[i][0]-5000-(2000*segmentNum)-500)/1000/dt) #500 represents 500 ms offset between each EEG segment.\n",
    "            dt2_index = int((timestamps[i][0]-3000-(2000*segmentNum))/1000/dt)\n",
    "            \n",
    "            noEvent_emg = data[dt1_index:dt2_index+1]-baselineCorrection_emg #Get EEG segment immediately prior to current event.\n",
    "           \n",
    "            #Normalize signal data WRT to max and find generate power spectral density \n",
    "            freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                                np.array([noEvent_emg]),\n",
    "                                                                samplingRate\n",
    "                                                                )\n",
    "            \n",
    "            if timestamps[i][0] < len(data)*1000*dt/2: #Check if timestamp is less half than total time of EMG data.\n",
    "                noEvent_emg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "                continue\n",
    "            noEvent_emg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "           \n",
    "        if i == len(timestamps): #If iteration reaches last event timestamp, set indices to get any possible EMG segment beyond timestamp.\n",
    "            numberOfSegments = int((len(data)*1000*dt/2-timestamps[i][0])/2000) #Calculate how many user-specified EMG segments can fit between two events.\n",
    "\n",
    "            for segmentNum in range(0, numberOfSegments):\n",
    "                #Add 500 ms between each EMG segment, except for segment closest in time to event\n",
    "                dt1_index = int((timestamps[i][0]+3000+(2000*segmentNum))/1000/dt)\n",
    "                dt2_index = int((timestamps[i][0]+5000+(2000*segmentNum)-500)/1000/dt) #500 represents 500 ms offset between each EEG segment.\n",
    "\n",
    "                noEvent_emg = data[dt1_index:dt2_index+1]  #Get emg segment \n",
    "                #Normalize signal data WRT to max and find generate power spectral density \n",
    "                freq_data, time_data, pwr_spectral_density_data = spectrogram( \n",
    "                                                                    np.array([noEvent_emg]),#/max(noEvent_emg)]), \n",
    "                                                                    samplingRate\n",
    "                                                                    )\n",
    "                if timestamps[i][0] < len(data)*1000*dt/2:\n",
    "                    noEvent_emg_PSD_train.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "                    continue\n",
    "                noEvent_emg_PSD_val.append(np.sort(np.sum(pwr_spectral_density_data[0],1))[-numberOfPSDComponents:None].tolist())\n",
    "        \n",
    "    return noEvent_emg_PSD_train, noEvent_emg_PSD_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create datasets, train model, and evaluate trained model.\n",
    "\n",
    "def createDatasets(brakingEvent_emg_PSD_train, noEvent_emg_PSD_train, brakingEvent_emg_PSD_val, noEvent_emg_PSD_val):\n",
    "    #Label = 0 indicates no event; label = 1 indicates EMG braking event\n",
    "    trainData = np.concatenate((brakingEvent_emg_PSD_train, noEvent_emg_PSD_train))\n",
    "    trainLabels_event = np.ones(len(brakingEvent_emg_PSD_train),dtype=int) \n",
    "    trainLabels_noEvent = np.zeros(len(noEvent_emg_PSD_train),dtype=int) \n",
    "    trainLabels = np.concatenate((trainLabels_event, trainLabels_noEvent))\n",
    "\n",
    "    valData = np.concatenate((brakingEvent_emg_PSD_val, noEvent_emg_PSD_val))\n",
    "    valLabels_event = np.ones(len(brakingEvent_emg_PSD_val),dtype=int) \n",
    "    valLabels_noEvent = np.zeros(len(noEvent_emg_PSD_val),dtype=int) \n",
    "    valLabels = np.concatenate((valLabels_event, valLabels_noEvent))\n",
    "    \n",
    "    return trainData, trainLabels, valData, valLabels\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def trainModel(trainData, trainLabels):\n",
    "    #LDA description: https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "    model = LinearDiscriminantAnalysis() \n",
    "    model.fit(np.array(trainData), trainLabels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluateModel(model, valData, valLabels):\n",
    "    #Validate model\n",
    "    eventPredictions = model.predict(valData).tolist()\n",
    "    #Accuracy as percent of correct predictions out of total predictions.\n",
    "    #accuracy = abs(sum(eventPredictions-valLabels))/len(eventPredictions)\n",
    "    \n",
    "    #Area under the curve score\n",
    "    AUCaccuracy = roc_auc_score(valLabels, model.predict_proba(np.array(valData))[:, 1])\n",
    "    \n",
    "    return AUCaccuracy #,accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test subject data:  11% 2/18 [00:14<01:56,  7.28s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (truncated file: eof = 186664186, sblock->base_addr = 512, stored_eof = 757773754)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m pbar1\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing test subject data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m pbar1:\n\u001b[0;32m---> 16\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    Read, sort and assign experimental to variables for: \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    signal channel names: cnt.clab\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    sampling frequency: cnt.fs\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    time-series data: cnt.x\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     cnt \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[1;32m    525\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    527\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    528\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    529\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    530\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    531\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    532\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 533\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    225\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 226\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    228\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (truncated file: eof = 186664186, sblock->base_addr = 512, stored_eof = 757773754)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Find all paths for the test subject data.\n",
    "testSubjectDataFilePaths =  glob('../EMG_Dataset_Haufe/*.mat')\n",
    "\n",
    "allTestSubjectAUCs = []\n",
    "\n",
    "numberOfPSDComponents = 129 #Maximum number of components for samplingRate = 200 hz.\n",
    "\n",
    "pbar1 = tqdm(testSubjectDataFilePaths)\n",
    "pbar1.set_description('Processing test subject data')\n",
    "for path in pbar1:\n",
    "    f = h5py.File(path,'r')\n",
    "    '''\n",
    "    Read, sort and assign experimental to variables for: \n",
    "    signal channel names: cnt.clab\n",
    "    sampling frequency: cnt.fs\n",
    "    time-series data: cnt.x\n",
    "    '''\n",
    "    cnt = f.get('cnt')\n",
    "    cnt.clab = np.array(cnt['clab'])\n",
    "    cnt.fs = np.array(cnt['fs'])\n",
    "    cnt.x = np.array(cnt['x']) \n",
    "\n",
    "    samplingRate = cnt.fs[0][0] #Down-/upsample rate for all data = 200Hz.\n",
    "    \n",
    "    #Read data for events corresponding to experimental data.\n",
    "    mrk = f.get('mrk')\n",
    "    mrk.classNames = mrk['className']\n",
    "    mrk.time = mrk['time']\n",
    "    mrk.y = mrk['y']\n",
    "    mrk.events = mrk['event']\n",
    "    #Find all car braking events (brake lights of lead vehicle turn on) and store corresponding timestamps.\n",
    "    carBrakeTime = []\n",
    "    for i in range(0, len(mrk.y)):\n",
    "        if mrk.y[i][1] == 1: #Check if car is braking, i.e. y[i] = 1\n",
    "            carBrakeTime.append(mrk.time[i][0]) #Store timestamp \n",
    "            \n",
    "    #Create train and validation datasets\n",
    "    brakingEvent_emg_PSD_train = []\n",
    "    brakingEvent_emg_PSD_val = []\n",
    "\n",
    "    noEvent_emg_PSD_train = []\n",
    "    noEvent_emg_PSD_val = []\n",
    "        \n",
    "    data = cnt.x[61] #Channel 61 for EMG of tibialis anterior\n",
    "    event_emg_PSD_train, event_emg_PSD_val = createDatasetFromEMGEvents(carBrakeTime, \n",
    "                                                                        data, \n",
    "                                                                        samplingRate, \n",
    "                                                                        numberOfPSDComponents)\n",
    "    _noEvent_emg_PSD_train, _noEvent_emg_PSD_val = createDatasetFromEMGWithoutEvents(mrk.time, \n",
    "                                                                                     data, \n",
    "                                                                                     samplingRate, \n",
    "                                                                                     numberOfPSDComponents)\n",
    "\n",
    "    for array in event_emg_PSD_train: brakingEvent_emg_PSD_train.append(array)\n",
    "    for array in event_emg_PSD_val: brakingEvent_emg_PSD_val.append(array)\n",
    "    for array in _noEvent_emg_PSD_train: noEvent_emg_PSD_train.append(array)\n",
    "    for array in _noEvent_emg_PSD_val: noEvent_emg_PSD_val.append(array)\n",
    "        \n",
    "    trainData, trainLabels, valData, valLabels = createDatasets(brakingEvent_emg_PSD_train,\n",
    "                                                                noEvent_emg_PSD_train,\n",
    "                                                                brakingEvent_emg_PSD_val,\n",
    "                                                                noEvent_emg_PSD_val)\n",
    "    trainedModel = trainModel(trainData, trainLabels)\n",
    "    AUCaccuracy = evaluateModel(trainedModel, valData, valLabels)\n",
    "    allTestSubjectAUCs.append(AUCaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grandAverageAUC = mean(allTestSubjectAUCs)\n",
    "print(\"LDA model AUC accuracy: \", round(grandAverageAUC,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
